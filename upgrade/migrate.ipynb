{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from minio import Minio\n",
    "from datetime import datetime\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_with_dates(json_str):\n",
    "    data = json.loads(json_str)\n",
    "    # parse dates if needed\n",
    "    return data\n",
    "\n",
    "df = pd.read_csv(\"datasets.csv\")\n",
    "\n",
    "df['versions'] = df['versions'].apply(json.loads)\n",
    "df['files'] = df['files'].apply(json.loads)\n",
    "df['folders'] = df['folders'].apply(json.loads)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "\tendpoint=os.getenv('S3_ENDPOINT'),\n",
    "\taccess_key=os.getenv('ACCESS_KEY_ID'),\n",
    "\tsecret_key=os.getenv('SECRET_ACCESS_KEY'),\n",
    "\tsecure=True,\n",
    ")\n",
    "\n",
    "old_bucket = os.getenv('OLD_BUCKET')\n",
    "new_bucket = os.getenv('NEW_BUCKET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version_id': 1, 'createdAt': '2023-11-02T13:11:36.142000', 'size': 17812745}\n",
      "{'version_id': 2, 'createdAt': '2023-11-02T13:11:36.142000', 'size': 17825361}\n",
      "{'version_id': 3, 'createdAt': '2023-11-04T15:08:47.877000', 'size': 21477259}\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import geopandas as gpd\n",
    "import random\n",
    "import rasterio as rio\n",
    "\n",
    "files_map = []\n",
    "for row in df.iterrows():\n",
    "\tdataset_id = row[1]['id']\n",
    "\tdataset_name = row[1]['name']\n",
    "\tversions = row[1]['versions']\n",
    "\tfiles = row[1]['files']\n",
    "\tfolders = row[1]['folders']\n",
    "\t# print(dataset_id, dataset_name, versions, files, folders)\n",
    "\t# print(dataset_id, json.loads(files))\n",
    "\tif len(json.loads(versions)) <= 1:\n",
    "\t\tcontinue\n",
    "\tfor version in json.loads(versions):\n",
    "\t\tprint(version)\n",
    "\t\tdata = []\n",
    "\t\tfor file in json.loads(files):\n",
    "\t\t\tif not version['version_id'] in file['versions']:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# print(file)\n",
    "\t\t\titem_id = file['name']\n",
    "\t\t\tif file['version'] > 1:\n",
    "\t\t\t\t\titem_id = f'{file[\"name\"]}-{random.randint(1, 1000000)}'\n",
    "\t\t\t\t\tprint(file['name'], '->', item_id)\n",
    "\t\t\tstac_item  = {\n",
    "\t\t\t\t'type': 'Feature',\n",
    "\t\t\t\t'stac_version': '1.0.0',\n",
    "\t\t\t\t'stac_extensions': [],\n",
    "\t\t\t\t'datetime': datetime.now(),  # must be native timestamp (https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp)\n",
    "\t\t\t\t'id': item_id,\n",
    "\t\t\t\t'bbox': {\n",
    "\t\t\t\t\t'xmin': 0.0,\n",
    "\t\t\t\t\t'ymin': 0.0,\n",
    "\t\t\t\t\t'xmax': 0.0,\n",
    "\t\t\t\t\t'ymax': 0.0\n",
    "\t\t\t\t}, # infer from file or from list of geometries\n",
    "\t\t\t\t'geometry': Polygon(), # empty polygon\n",
    "\t\t\t\t'assets': { 'asset': { # STAC needs this to be a Dict[str, Asset], not list !!! use same key or parquet breaks !!!\n",
    "\t\t\t\t\t'href': f'https://dev.api.eotdl.com/datasets/{dataset_id}/stage/{item_id}', # TODO: change to prod\n",
    "\t\t\t\t\t'checksum': file['checksum'],\n",
    "\t\t\t\t\t'timestamp': file['createdAt'],\n",
    "\t\t\t\t\t'size': file['size'],\n",
    "\t\t\t\t}},\n",
    "\t\t\t\t\"links\": [],\n",
    "\t\t\t\t# 'collection': 'source',\n",
    "\t\t\t\t# anything below are properties (need at least one!)\n",
    "\t\t\t\t'repository': 'eotdl',\t\t\t\t\n",
    "\t\t\t}\n",
    "\t\t\t# Check if file is a tif/tiff\n",
    "\t\t\tif file['name'].lower().endswith(('.tif', '.tiff')):\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\t# Get presigned URL for reading file\n",
    "\t\t\t\t\tpresigned_url = minio_client.get_presigned_url(\n",
    "\t\t\t\t\t\t\"GET\",\n",
    "\t\t\t\t\t\told_bucket,\n",
    "\t\t\t\t\t\tf'{dataset_id}/{file[\"name\"]}_{file[\"version\"]}'\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\t# Read with rasterio using presigned URL\n",
    "\t\t\t\t\twith rio.open(presigned_url) as src:\n",
    "\t\t\t\t\t\tbounds = src.bounds\n",
    "\t\t\t\t\t\tstac_item['bbox'] = {\n",
    "\t\t\t\t\t\t\t'xmin': bounds.left,\n",
    "\t\t\t\t\t\t\t'ymin': bounds.bottom, \n",
    "\t\t\t\t\t\t\t'xmax': bounds.right,\n",
    "\t\t\t\t\t\t\t'ymax': bounds.top\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t\tstac_item['geometry'] = Polygon([\n",
    "\t\t\t\t\t\t\t[bounds.left, bounds.bottom],\n",
    "\t\t\t\t\t\t\t[bounds.left, bounds.top],\n",
    "\t\t\t\t\t\t\t[bounds.right, bounds.top], \n",
    "\t\t\t\t\t\t\t[bounds.right, bounds.bottom],\n",
    "\t\t\t\t\t\t\t[bounds.left, bounds.bottom]\n",
    "\t\t\t\t\t\t])\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\t# If reading fails, keep empty bbox/geometry\n",
    "\t\t\t\t\tpass\n",
    "\t\t\tdata.append(stac_item)\n",
    "\t\t\t# copy file from old bucket to new bucket\n",
    "\t\t\t# minio_client.fget_object(\n",
    "\t\t\t# \told_bucket,\n",
    "\t\t\t# \tf'{dataset_id}/{file['name']}_{file['version']}',\n",
    "\t\t\t# \tf'{dataset_id}/{item_id}'\n",
    "\t\t\t# )\n",
    "\t\t\t# minio_client.fput_object(\n",
    "\t\t\t# \tnew_bucket,\n",
    "\t\t\t# \tf'{dataset_id}/{item_id}',\n",
    "\t\t\t# \tf'{dataset_id}/{item_id}'\n",
    "\t\t\t# )\n",
    "\t\t\tfiles_map.append((f'{dataset_id}/{file['name']}_{file['version']}', f'{dataset_id}/{item_id}'))\n",
    "\t\tif data:\n",
    "\t\t\tgdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "\t\t\tcatalog_name = f'catalog.v{version[\"version_id\"]}.parquet'\n",
    "\t\t\tgdf.to_parquet(catalog_name)\n",
    "\t\t\t# copy parquet to bucket\n",
    "\t\t\tminio_client.fput_object(\n",
    "\t\t\t\tnew_bucket,\n",
    "\t\t\t\tf'{dataset_id}/{catalog_name}',\n",
    "\t\t\t\tcatalog_name\n",
    "\t\t\t)\n",
    "\tbreak\n",
    "\t\n",
    "_df = pd.DataFrame(files_map, columns=['old_path', 'new_path'])\n",
    "_df.to_csv('files_map.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = pd.read_csv('files_map.csv')\n",
    "# _df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = gpd.read_parquet('catalog.v3.parquet')\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = minio_client.list_objects(new_bucket)\n",
    "# for file in files:\n",
    "# \tprint(file.object_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in bucket/645a26564d1c8b7b364ee631\n",
    "\n",
    "# files = minio_client.list_objects(new_bucket, '6543ba3f68ea46a6677efec9', recursive=True)\n",
    "# for file in files:\n",
    "# \tprint(file.object_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

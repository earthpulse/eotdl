<script>
	import Api from "../../components/Api.svelte";
	import CLI from "../../components/CLI.svelte";
	import UI from "../../components/UI.svelte";
	import Code from "../../components/Code.svelte";
	import BadgeWarning from "../../BadgeWarning.svelte";
</script>

<BadgeWarning>Currently, only Q0 datasets are supported.</BadgeWarning>

Learn more about the datasets [quality levels](/docs/datasets/quality).

# Ingest datasets

You can upload your own datasets to the EOTDL platform. 

The following constraints apply to the dataset name:
- It must be unique
- It must be between 3 and 45 characters long
- It can only contain alphanumeric characters and dashes.

Once a file is ingested, it cannot be deleted since other users may be using it in their own pipelines. However, you can update a new file with the same name to overwrite it.

> ⚠ Up until versioning is supported, this may also affect other users pipelines.

## User interface

<UI>In the <a href="/datasets" class="text-green-200 hover:underline">datasets</a> section, use the INGEST button to upload a dataset.</UI>

You'll have to select the files to upload and fill in the information about the dataset.

A limit of **10 files** per dataset is enforced. Additionally, the file size limit is **1 GB**. If you need to ingest larger files, please use the CLI.

You'll need to be logged in to ingest datasets.

You can use the `EDIT` button in the dataset page to update the information or data.

## CLI 

The CLI is the most convenient way to ingest large datasets. You can ingest a dataset using the following CLI command:

<CLI><Code>eotdl datasets ingest {'<'}dataset-path> {'<'}dataset-name></Code></CLI>

Where `dataset-path` is the path to a file or a folder containing multiple files. 

> ⚠ The CLI will NOT upload files recursively, so make sure to comporess subfolders with multiple files (zip, tar or tar.gz).

A limit of **10 files** per dataset is enforced. Additionally, the file size limit is **5 TB**. If you need to ingest larger files, please get in touch with us.

> After uploading a dataset with the CLI we recommend visiting the dataset page to fill in the rest of the information.


## API 

You can ingest a file to a dataset using the following API call:

<Api><Code>curl -X 'POST' \
  'https://api.eotdl.com/datasets' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer {'<'}your-token> \
  -H 'Content-Type: multipart/form-data' \
  -F 'file=@{'<'}file-path>;type=application/zip' \
  -F 'dataset={'<'}dataset-name>' \
  -F 'checksum={'<'}sha1 checksum (optional)>'</Code></Api>

The first time that you ingest a file to a dataset, the dataset is created. From that point on, you can upload more files to 
the same dataset up until a limit of **10 files**. 

> ⚠ The API can work well for small files, but for the ingestion of large datasets (> 1 GB) we recommend using the CLI.

You can also fill in the information about the dataset with the following API call:

<Api><Code>curl -X 'POST' \
  'https://api.eotdl.com/datasets/dataset-id' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer {'<'}your-token> \
  -H 'Content-Type: application/json' \
  -F 'file=@{'<'}dataset-path>;type=application/zip' \
  -d '{'{'}
    "name": "new-name",
    "description": "dataset description",
    "tags": [
      "tag1",
      "tag2"
    ],
    "author": "autho nmae",
    "link": "link to source",
    "license": "dataset license"
  }'</Code></Api>

In this case, all fields are optional.

You can retrieve a list of valid tags with the following API call:


<Api><Code>curl -X 'GET' \
  'https://api.eotdl.com/tags' \
  -H 'accept: application/json'</Code></Api>
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to train a Transformer model in an unsupervised way.\n",
    "\n",
    "The code is based on https://github.com/Clay-foundation/model\n",
    "\n",
    "> Clay’s model takes satellite imagery, along with information about location and time, as an input, and outputs embeddings, which are mathematical representations of a given area at a certain time on Earth’s surface. It uses a Vision Transformer architecture adapted to understand geospatial and temporal relations on Earth Observation data. The model is trained via Self-supervised learning (SSL) using a Masked Autoencoder (MAE) method. It supports inputs of any size and any number of bands. The output embeddings can be used to train models for downstream tasks, or the entire model can be fine-tuned for a specific task.\n",
    "\n",
    "For this use case, we pre-train the model with Satellogic, Sentinel-2 and Sentinel-1 data. However, the method is general and can be applied to other datasets.\n",
    "\n",
    "Components of Clay v1.5:\n",
    "- Dynamic Embedding Block: This component generates patches for the chips from the number of bands and their wavelengths, which are then fed into the masked autoencoder (MAE).\n",
    "- Position Encoding: This component encodes spatial and temporal information by adding positional encoding to the model. This encoding is scaled according to the Ground Sampling Distance (GSD) and is combined with location information (latitude/longitude) and time step (week/hour).\n",
    "- Masked Autoencoder (MAE): A VIT-based MAE is used to reconstruct the sensor data for all input bands. This contributes to 95% of the total loss, known as the reconstruction loss.\n",
    "- Teacher: DINOv2 is used as a teacher to compute the representation loss, which accounts for the remaining 5% of the total loss.\n",
    "\n",
    "The pre-trained model can process stacks of geospatial data from different sensors with various resolutions and bands, and output vector embeddings. During pre-training, the model processes stacks of chips from different sensors along with metadata such as wavelengths, GSD, latitude/longitude, and time step. The task involves capturing spatial, temporal, and spectral information about Earth and representing these relationships in high-dimensional space. Each resulting embedding represents a specific area of Earth at a particular time.\n",
    "\n",
    "Clay v1.5 was trained on 70 million globally distributed chips of size 156x256, collected according to the land use/land cover (LULC) statistics of the globe. The training was conducted on AWS using 20 g6.48xlarge instances (8 L4 each) for ~100 epochs in Sep 2024 (~8h per epoch).\n",
    "\n",
    "In this use case, we train with significantly less data and a smaller model just as a demonstration, however the method is general and can be scaled to any dataset if resources are available.\n",
    "\n",
    "![](https://github.com/Clay-foundation/model/assets/8049519/f6a1e92c-3993-4148-98a2-e3805dae4414)\n",
    "\n",
    "Clay v1.5’s architecture includes a dynamic embedding block for generating patches from multi-band inputs, position encoding to integrate spatial and temporal data, a Vision Transformer-based masked autoencoder (MAE) for reconstructing sensor data, and a DINOv2 teacher model to enhance representation learning. This architecture allows the model to process inputs from various satellite sensors of any size and number of bands, capturing complex geospatial information effectively.\n",
    "\n",
    "Input modalities:\n",
    "- A fixed specification of 10 bands from Sentinel-2, 6 bands from Landsat, 4 bands from NAIP, 3 bands from LINZ, 2 bands from Sentinel-1 data and 7 bands from MODIS.\n",
    "\n",
    "Output modalities:\n",
    "- As a masked auto-encoder, it has a fixed specification of 10 bands from Sentinel-2, 6 bands from Landsat, 4 bands from NAIP, 3 bands from LINZ, 2 bands from Sentinel-1 data and 7 bands from MODIS to closely mimic the input.\n",
    "\n",
    "Model size (in millions):\n",
    "- Number of parameters: 632M\n",
    "- Encoder size: 311M\n",
    "- Decoder size: 15M\n",
    "- Teacher size: 304M\n",
    "- Model size on disk (just the encoder): 1.25 GB."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

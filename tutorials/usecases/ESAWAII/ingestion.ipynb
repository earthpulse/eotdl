{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Polygon\n",
    "import pyarrow.parquet as pq\n",
    "import pystac\n",
    "from tqdm import tqdm\n",
    "import stac_geoparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a list of the files ingested in ESA PRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1533285</td>\n",
       "      <td>esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>185044694</td>\n",
       "      <td>esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168779279</td>\n",
       "      <td>esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144352380</td>\n",
       "      <td>esawaai/2019/08/27/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>209491051</td>\n",
       "      <td>esawaai/2019/08/27/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18484</th>\n",
       "      <td>185157726</td>\n",
       "      <td>esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18485</th>\n",
       "      <td>169103739</td>\n",
       "      <td>esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18486</th>\n",
       "      <td>129979939</td>\n",
       "      <td>esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18487</th>\n",
       "      <td>184753718</td>\n",
       "      <td>esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18488</th>\n",
       "      <td>168782931</td>\n",
       "      <td>esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18489 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            size                                               path\n",
       "0        1533285  esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...\n",
       "1      185044694  esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...\n",
       "2      168779279  esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...\n",
       "3      144352380  esawaai/2019/08/27/S1A_IW_ESAWAAI__1SDV_201908...\n",
       "4      209491051  esawaai/2019/08/27/S1A_IW_ESAWAAI__1SDV_201908...\n",
       "...          ...                                                ...\n",
       "18484  185157726  esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...\n",
       "18485  169103739  esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...\n",
       "18486  129979939  esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...\n",
       "18487  184753718  esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...\n",
       "18488  168782931  esawaai/2023/11/04/S1A_IW_ESAWAAI__1SDV_202311...\n",
       "\n",
       "[18489 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read file and process lines\n",
    "with open('esawaai.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Remove leading/trailing whitespace and split on first space\n",
    "data = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:  # Skip empty lines\n",
    "        size, path = line.split(' ', 1)\n",
    "        data.append([int(size), path])\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['size', 'path'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a STAC-compliant parquet file. We should add here all the required metadata (PRR updates later are not supported)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>stac_version</th>\n",
       "      <th>stac_extensions</th>\n",
       "      <th>datetime</th>\n",
       "      <th>id</th>\n",
       "      <th>bbox</th>\n",
       "      <th>geometry</th>\n",
       "      <th>assets</th>\n",
       "      <th>links</th>\n",
       "      <th>repository</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Feature</td>\n",
       "      <td>1.0.0</td>\n",
       "      <td>[]</td>\n",
       "      <td>2025-04-08 13:13:44.341092</td>\n",
       "      <td>esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...</td>\n",
       "      <td>{'xmin': 0.0, 'ymin': 0.0, 'xmax': 0.0, 'ymax'...</td>\n",
       "      <td>POLYGON EMPTY</td>\n",
       "      <td>{'asset': {'href': 'https://...', 'checksum': ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>eotdl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      type stac_version stac_extensions                   datetime  \\\n",
       "0  Feature        1.0.0              [] 2025-04-08 13:13:44.341092   \n",
       "\n",
       "                                                  id  \\\n",
       "0  esawaai/2019/08/26/S1A_IW_ESAWAAI__1SDV_201908...   \n",
       "\n",
       "                                                bbox       geometry  \\\n",
       "0  {'xmin': 0.0, 'ymin': 0.0, 'xmax': 0.0, 'ymax'...  POLYGON EMPTY   \n",
       "\n",
       "                                              assets links repository  \n",
       "0  {'asset': {'href': 'https://...', 'checksum': ...    []      eotdl  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "for _, row in df.iterrows():\n",
    "\t# print(row['path'])\n",
    "\tsize = row['size']\n",
    "\titem_id = row['path'] # Do we want to use the path as the item id?\n",
    "\tasset_href = 'https://...' # TODO\n",
    "\tchecksum = None # TODO\n",
    "\tdata.append({\n",
    "\t\t'type': 'Feature',\n",
    "\t\t'stac_version': '1.0.0',\n",
    "\t\t'stac_extensions': [],\n",
    "\t\t'datetime': datetime.now(),  # must be native timestamp (https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#timestamp)\n",
    "\t\t'id': item_id,\n",
    "\t\t'bbox': {\n",
    "\t\t\t'xmin': 0.0,\n",
    "\t\t\t'ymin': 0.0,\n",
    "\t\t\t'xmax': 0.0,\n",
    "\t\t\t'ymax': 0.0\n",
    "\t\t}, \n",
    "\t\t'geometry': Polygon(), # empty polygon\n",
    "\t\t'assets': { 'asset': { \n",
    "\t\t\t'href': asset_href,\n",
    "\t\t\t'checksum': checksum,\n",
    "\t\t\t'timestamp': datetime.now(),\n",
    "\t\t\t'size': size,\n",
    "\t\t}},\n",
    "\t\t\"links\": [],\n",
    "\t\t# anything below are properties (need at least one!)\n",
    "\t\t'repository': 'eotdl',\t\t\n",
    "\t})\n",
    "\tbreak\n",
    "\n",
    "gdf = gpd.GeoDataFrame(data, geometry='geometry')\n",
    "gdf.to_parquet('catalog.parquet')\n",
    "gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the parquet file has been generated, we can convert to STAC items and POST to PRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 184.09it/s]\n"
     ]
    }
   ],
   "source": [
    "table = pq.read_table('catalog.parquet')\n",
    "items = []\n",
    "for item in tqdm(stac_geoparquet.arrow.stac_table_to_items(table), total=len(table)):\n",
    "\titem = pystac.Item.from_dict(item)\n",
    "\t# TODO: POST to PRR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the `catalog.parquet` file is correct, we can ingest directly into EOTDL. Otherwise, update the file before ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'README.md'\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Error loading metadata",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/files/ingest.py:109\u001b[39m, in \u001b[36mingest\u001b[39m\u001b[34m(path, repo, retrieve, mode, user)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \treadme = \u001b[43mfrontmatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mREADME.md\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \tmetadata_dict = readme.metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/frontmatter/__init__.py:161\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(fd, encoding, handler, **defaults)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    162\u001b[39m         text = f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen codecs>:918\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(filename, mode, encoding, errors, buffering)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'README.md'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meotdl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ingest_dataset_catalog\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# TODO: create README.md\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mingest_dataset_catalog\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/datasets/ingest.py:47\u001b[39m, in \u001b[36mingest_dataset_catalog\u001b[39m\u001b[34m(path, logger)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mingest_dataset_catalog\u001b[39m(path, logger=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     46\u001b[39m \tpath = Path(path)\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mingest_catalog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDatasetsAPIRepo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieve_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/files/ingest.py:104\u001b[39m, in \u001b[36mingest_catalog\u001b[39m\u001b[34m(path, repo, retrieve, mode)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mingest_catalog\u001b[39m(path, repo, retrieve, mode):\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mingest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/auth/auth.py:61\u001b[39m, in \u001b[36mwith_auth.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     60\u001b[39m     user = auth()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/files/ingest.py:116\u001b[39m, in \u001b[36mingest\u001b[39m\u001b[34m(path, repo, retrieve, mode, user)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    115\u001b[39m \t\u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \t\u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mError loading metadata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# retrieve dataset (create if doesn't exist)\u001b[39;00m\n\u001b[32m    118\u001b[39m dataset_or_model = retrieve(metadata, user)\n",
      "\u001b[31mException\u001b[39m: Error loading metadata"
     ]
    }
   ],
   "source": [
    "from eotdl.datasets import ingest_dataset_catalog\n",
    "\n",
    "# TODO: create README.md\n",
    "\n",
    "ingest_dataset_catalog('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now access the dataset in EOTDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eotdl.datasets import retrieve_datasets\n",
    "\n",
    "retrieve_datasets('ESAWAII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Dataset doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meotdl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stage_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mstage_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mESAWAII\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/auth/auth.py:61\u001b[39m, in \u001b[36mwith_auth.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     60\u001b[39m     user = auth()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/datasets/stage.py:23\u001b[39m, in \u001b[36mstage_dataset\u001b[39m\u001b[34m(dataset_name, version, path, logger, assets, force, verbose, user, file)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;129m@with_auth\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstage_dataset\u001b[39m(\n\u001b[32m     13\u001b[39m     dataset_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     file=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     22\u001b[39m ):\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     dataset = \u001b[43mretrieve_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     25\u001b[39m         version = \u001b[38;5;28msorted\u001b[39m([v[\u001b[33m'\u001b[39m\u001b[33mversion_id\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m dataset[\u001b[33m\"\u001b[39m\u001b[33mversions\u001b[39m\u001b[33m\"\u001b[39m]])[-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/eotdl/eotdl/datasets/retrieve.py:17\u001b[39m, in \u001b[36mretrieve_dataset\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m     15\u001b[39m data, error = repo.retrieve_dataset(name)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[31mException\u001b[39m: Dataset doesn't exist"
     ]
    }
   ],
   "source": [
    "from eotdl.datasets import stage_dataset\n",
    "\n",
    "stage_dataset('ESAWAII', path='output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file 'output/catalog.v1.parquet'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/geopandas/io/arrow.py:653\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    652\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.schema\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1371\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[39m\n\u001b[32m   1368\u001b[39m     partitioning = ds.HivePartitioning.discover(\n\u001b[32m   1369\u001b[39m         infer_dictionary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28mself\u001b[39m._dataset = \u001b[43mds\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mparquet_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:794\u001b[39m, in \u001b[36mdataset\u001b[39m\u001b[34m(source, schema, format, filesystem, partitioning, partition_base_dir, exclude_invalid_files, ignore_prefixes)\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(source):\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_filesystem_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(source, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:476\u001b[39m, in \u001b[36m_filesystem_dataset\u001b[39m\u001b[34m(source, schema, filesystem, partitioning, format, partition_base_dir, exclude_invalid_files, selector_ignore_prefixes)\u001b[39m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     fs, paths_or_selector = \u001b[43m_ensure_single_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m options = FileSystemFactoryOptions(\n\u001b[32m    479\u001b[39m     partitioning=partitioning,\n\u001b[32m    480\u001b[39m     partition_base_dir=partition_base_dir,\n\u001b[32m    481\u001b[39m     exclude_invalid_files=exclude_invalid_files,\n\u001b[32m    482\u001b[39m     selector_ignore_prefixes=selector_ignore_prefixes\n\u001b[32m    483\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/dataset.py:441\u001b[39m, in \u001b[36m_ensure_single_source\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(path)\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filesystem, paths_or_selector\n",
      "\u001b[31mFileNotFoundError\u001b[39m: output/catalog.v1.parquet",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m catalog = \u001b[43mgpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput/catalog.v1.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m catalog\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/geopandas/io/arrow.py:751\u001b[39m, in \u001b[36m_read_parquet\u001b[39m\u001b[34m(path, columns, storage_options, bbox, **kwargs)\u001b[39m\n\u001b[32m    747\u001b[39m filesystem, path = _get_filesystem_path(\n\u001b[32m    748\u001b[39m     path, filesystem=filesystem, storage_options=storage_options\n\u001b[32m    749\u001b[39m )\n\u001b[32m    750\u001b[39m path = _expand_user(path)\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m schema, metadata = \u001b[43m_read_parquet_schema_and_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    753\u001b[39m geo_metadata = _validate_and_decode_metadata(metadata)\n\u001b[32m    755\u001b[39m bbox_filter = (\n\u001b[32m    756\u001b[39m     _get_parquet_bbox_filter(geo_metadata, bbox) \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    757\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/geopandas/io/arrow.py:655\u001b[39m, in \u001b[36m_read_parquet_schema_and_metadata\u001b[39m\u001b[34m(path, filesystem)\u001b[39m\n\u001b[32m    653\u001b[39m     schema = parquet.ParquetDataset(path, filesystem=filesystem, **kwargs).schema\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     schema = \u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m metadata = schema.metadata\n\u001b[32m    659\u001b[39m \u001b[38;5;66;03m# read metadata separately to get the raw Parquet FileMetaData metadata\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;66;03m# (pyarrow doesn't properly exposes those in schema.metadata for files\u001b[39;00m\n\u001b[32m    661\u001b[39m \u001b[38;5;66;03m# created by GDAL - https://issues.apache.org/jira/browse/ARROW-16688)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:2342\u001b[39m, in \u001b[36mread_schema\u001b[39m\u001b[34m(where, memory_map, decryption_properties, filesystem)\u001b[39m\n\u001b[32m   2340\u001b[39m file_ctx = nullcontext()\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m filesystem \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     file_ctx = where = \u001b[43mfilesystem\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen_input_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2344\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m file_ctx:\n\u001b[32m   2345\u001b[39m     file = ParquetFile(\n\u001b[32m   2346\u001b[39m         where, memory_map=memory_map,\n\u001b[32m   2347\u001b[39m         decryption_properties=decryption_properties)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/_fs.pyx:789\u001b[39m, in \u001b[36mpyarrow._fs.FileSystem.open_input_file\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/eotdl/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] Failed to open local file 'output/catalog.v1.parquet'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "catalog = gpd.read_parquet('output/catalog.v1.parquet')\n",
    "catalog"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
